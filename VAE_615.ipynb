{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_615.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1thrlhNmcusSBm4PXTw1IVDeOfFemuFf-",
      "authorship_tag": "ABX9TyP+g/H8JOGryhx1JAXojFiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hereagain-Y/TF_tutorial/blob/main/VAE_615.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7-ylF3xzCcZm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/DL/VAE/rep_00148.tsv\",\n",
        "            sep=\"\\t\")\n",
        "comp_seq=data[\"amino_acid\"].tolist()\n",
        "seq='IHTGCYRTQYNQGNK'\n",
        "print(comp_seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDcHSy4BCkUW",
        "outputId": "7b98fa20-9ed2-4a25-d17c-efaa1198a0e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['WPPHRNQASTKCLNNMQ', 'PLQEDGMNNDH', 'MFRQLIPEIIC', 'TLRQMCHKHKVV', 'SNRWDGRSIWHVDKAYH', 'EHMCIQGNFLAGLVS', 'YITNHHNVPLIAFWY', 'NYLHGNGSWSVSGIPF', 'YIAFWMIDMG', 'LAEEMGRCPI', 'PCRWRHCEHTL', 'CEPWAGGVKYK', 'KDQGVWCHNLPDKYI', 'VSHCDKCREGQAISDMEHS', 'QCKFCTRDCWDSG', 'DYGGVGFTNCMY', 'VNDEMDWETQHGGVHQKT', 'RDCYQIKPHDINPTYE', 'SMKVQDFPTPSC', 'RWCLVKRIGHCMD', 'HMLQFHYQRGT', 'DPVECCWHLQLE', 'MTHRANSCGSK', 'TETIFIHWFSNIRQE', 'YVKLKTPLKGYFRYYIE', 'ETGMTHNWEVWSLHDYI', 'YYQIRGYRKDVMM', 'AHGNMAVKHSRDKPTWYM', 'TKLPRVTFWICGQPER', 'QSMSTMFAMKKQ', 'WCLHEQDSTWRWVAVNLE', 'YLYEATLMKR', 'GSCHIPMKYCWHTRGCFHN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ONE_HOT = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
        "                   'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}"
      ],
      "metadata": {
        "id": "4QFoT0kyCyNq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tumorCDR3 data"
      ],
      "metadata": {
        "id": "UWTUQARYGZR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = []\n",
        "my_file = open(\"/content/drive/My Drive/DL/VAE/NormalCDR3.txt\", \"r\")\n",
        "content_list = my_file.read().splitlines()\n",
        "content_list=np.array(content_list)\n",
        "print(content_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sdzy4Q9GYu3",
        "outputId": "3f9a8ef3-5ba9-43fe-a302-ba388390faf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CASSLKPNTEAFF' 'CASSAHIANYGYTF' 'CASSPRPNTEAFF' ... 'CASSVDVGYEQYF'\n",
            " 'CASRLAGQETQYF' 'CASSVVPNTEAFF']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d= {} # bipphysics dictionary\n",
        "with open(\"/content/drive/My Drive/DL/VAE/AAidx_PCA.txt\") as f:\n",
        "    next(f)\n",
        "    for line in f.readlines():\n",
        "        line=line.strip().split('\\t') #\n",
        "        AA=line[0]\n",
        "        tag=0\n",
        "        values=[]\n",
        "        for PC in line[1:]:\n",
        "            values.append(float(PC))\n",
        "        if tag==1: \n",
        "            continue\n",
        "        d[AA]=values\n",
        "d # AA\n"
      ],
      "metadata": {
        "id": "aP7LBLHxC49D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max length\n",
        "#comp_seq=data[\"amino_acid\"].tolist()\n",
        "comp_seq=content_list\n",
        "print(comp_seq)\n",
        "max_len=-1 \n",
        "\n",
        "for AA in comp_seq:\n",
        "    if len(AA)>max_len:\n",
        "        max_len=len(AA)\n",
        "        #res=AA\n",
        "print(max_len )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxYBWURNC_9Z",
        "outputId": "bb705dcf-611e-4cb0-c00d-339ff9a188ae"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CASSLKPNTEAFF' 'CASSAHIANYGYTF' 'CASSPRPNTEAFF' ... 'CASSVDVGYEQYF'\n",
            " 'CASRLAGQETQYF' 'CASSVVPNTEAFF']\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PC_length=len(d['C'])\n",
        "def AAindexEncoding(Seq):\n",
        "    length_seq=len(Seq)\n",
        "    global max_len\n",
        "    AAE=np.zeros([max_len,15])\n",
        "    if length_seq<max_len:\n",
        "        for amino in range(length_seq):\n",
        "            AA=Seq[amino]# \n",
        "            AAE[amino,]=d[AA] # add PC value \n",
        "            \n",
        "        for amino in range(length_seq,max_len):\n",
        "            AAE[amino,]=np.zeros(15)\n",
        "    else: \n",
        "        for amino in range(length_seq):\n",
        "            AA=Seq[amino]# \n",
        "            AAE[amino,]=d[AA]\n",
        "        \n",
        "    #AAE=np.transpose(AAE.astype(np.float32)) # row as PC. and column as AA sequence \n",
        "    return AAE \n",
        "  \n",
        "def GetFeatures(file):\n",
        "    #sequence=file['amino_acid'].tolist()\n",
        "    #sequence=np.array(sequence)\n",
        "    #sequence = file.read().splitlines()\n",
        "    #sequence=np.array(sequence)\n",
        "    hot_encode=[]\n",
        "    for seq in file:\n",
        "        hot_encode.append(AAindexEncoding(seq))\n",
        "    hot_encode=np.array(hot_encode)\n",
        "    result=np.array(hot_encode)\n",
        "    return(result) # dimension: number of sequence [15*length(sequence)]\n",
        "\n",
        "\n",
        "r1=GetFeatures(content_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "5b3L9CNdDIb1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "nfKE5VwJDONG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "Q8pBPD42GEvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trabsform \n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
        "r1_transform=torch.from_numpy(r1)\n",
        "r1_transform=r1_transform.float()\n",
        "train_ds, test_ds = torch.utils.data.random_split(r1_transform, (30000, 10000))\n",
        "print(train_ds, test_ds)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=1000)\n",
        "test_loader  = DataLoader(dataset=test_ds,  batch_size=1000)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esy5kIOXGEht",
        "outputId": "d793e8be-c906-4942-f91e-5eb82f4a60e8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7f451c0c2150> <torch.utils.data.dataset.Subset object at 0x7f451c0c20d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import TensorDataset\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
        "#r1_transform=torch.Tensor(r1)\n",
        "\n",
        "train_ds, test_ds = torch.utils.data.random_split(r1, (30000, 10000))\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=1000)\n",
        "test_loader  = DataLoader(dataset=test_ds,  batch_size=1000)\n"
      ],
      "metadata": {
        "id": "ft5cw6VrxOE3"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set paramters:\n",
        "cuda = True\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "x_dim=285 # 19*15\n",
        "hidden_dim = 256\n",
        "latent_dim = 128\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "HizXfI1blIlq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "        \n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        \n",
        "        self.training = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_       = self.LeakyReLU(self.FC_input(x))\n",
        "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
        "        mean     = self.FC_mean(h_)\n",
        "        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance \n",
        "                                                       #             (i.e., parateters of simple tractable normal distribution \"q\"\n",
        "        \n",
        "        return mean, log_var"
      ],
      "metadata": {
        "id": "f6l50xr0F9BQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
        "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
        "        \n",
        "        x_hat = torch.sigmoid(self.FC_output(h))\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "_xKOYca8GGS0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "        \n",
        "    def reparameterization(self, mean, var):\n",
        "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
        "        z = mean + var*epsilon                          # reparameterization trick\n",
        "        return z\n",
        "        \n",
        "                \n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.Encoder(x)\n",
        "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var) ]\n",
        "        x_hat            = self.Decoder(z)\n",
        "        \n",
        "        return x_hat, mean, log_var"
      ],
      "metadata": {
        "id": "oftbvF8pGIEp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
      ],
      "metadata": {
        "id": "9GrhkqDjGJq-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    #reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "x9DQJN6wrjU-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training VAE...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    overall_loss = 0\n",
        "    for batch_idx, x in enumerate(train_loader):\n",
        "        x = x.view(1000, x_dim)\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mean, log_var = model(x)\n",
        "        loss = loss_function(x, x_hat, mean, log_var)\n",
        "        \n",
        "        overall_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size))\n",
        "    \n",
        "print(\"Finish!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQKesrMHGLLd",
        "outputId": "cb557500-7ea4-493a-ffee-6d7264f1b766"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training VAE...\n",
            "\tEpoch 1 complete! \tAverage Loss:  -56331.00468965517\n",
            "\tEpoch 2 complete! \tAverage Loss:  -56341.23241379311\n",
            "\tEpoch 3 complete! \tAverage Loss:  -56357.144\n",
            "\tEpoch 4 complete! \tAverage Loss:  -56396.44510344828\n",
            "\tEpoch 5 complete! \tAverage Loss:  -56409.03324137931\n",
            "\tEpoch 6 complete! \tAverage Loss:  -56396.169931034485\n",
            "\tEpoch 7 complete! \tAverage Loss:  -56424.94606896552\n",
            "\tEpoch 8 complete! \tAverage Loss:  -56414.80441379311\n",
            "\tEpoch 9 complete! \tAverage Loss:  -56449.91572413793\n",
            "\tEpoch 10 complete! \tAverage Loss:  -56456.1875862069\n",
            "\tEpoch 11 complete! \tAverage Loss:  -56463.82675862069\n",
            "\tEpoch 12 complete! \tAverage Loss:  -56472.17131034483\n",
            "\tEpoch 13 complete! \tAverage Loss:  -56495.485655172415\n",
            "\tEpoch 14 complete! \tAverage Loss:  -56523.58593103448\n",
            "\tEpoch 15 complete! \tAverage Loss:  -56568.054344827586\n",
            "\tEpoch 16 complete! \tAverage Loss:  -56648.31862068966\n",
            "\tEpoch 17 complete! \tAverage Loss:  -56696.35434482759\n",
            "\tEpoch 18 complete! \tAverage Loss:  -56713.16206896552\n",
            "\tEpoch 19 complete! \tAverage Loss:  -56723.781517241376\n",
            "\tEpoch 20 complete! \tAverage Loss:  -56708.94165517241\n",
            "\tEpoch 21 complete! \tAverage Loss:  -56700.06827586207\n",
            "\tEpoch 22 complete! \tAverage Loss:  -56730.72165517241\n",
            "\tEpoch 23 complete! \tAverage Loss:  -56732.73158620689\n",
            "\tEpoch 24 complete! \tAverage Loss:  -56753.146068965514\n",
            "\tEpoch 25 complete! \tAverage Loss:  -56761.164\n",
            "\tEpoch 26 complete! \tAverage Loss:  -56873.17075862069\n",
            "\tEpoch 27 complete! \tAverage Loss:  -56939.49462068966\n",
            "\tEpoch 28 complete! \tAverage Loss:  -56950.16220689655\n",
            "\tEpoch 29 complete! \tAverage Loss:  -57017.35931034483\n",
            "\tEpoch 30 complete! \tAverage Loss:  -57080.56427586207\n",
            "\tEpoch 31 complete! \tAverage Loss:  -57074.45006896552\n",
            "\tEpoch 32 complete! \tAverage Loss:  -57094.9315862069\n",
            "\tEpoch 33 complete! \tAverage Loss:  -57108.21448275862\n",
            "\tEpoch 34 complete! \tAverage Loss:  -57111.01655172414\n",
            "\tEpoch 35 complete! \tAverage Loss:  -57138.69724137931\n",
            "\tEpoch 36 complete! \tAverage Loss:  -57140.801655172414\n",
            "\tEpoch 37 complete! \tAverage Loss:  -57161.219172413796\n",
            "\tEpoch 38 complete! \tAverage Loss:  -57197.89655172414\n",
            "\tEpoch 39 complete! \tAverage Loss:  -57220.40427586207\n",
            "\tEpoch 40 complete! \tAverage Loss:  -57275.14372413793\n",
            "\tEpoch 41 complete! \tAverage Loss:  -57289.66731034483\n",
            "\tEpoch 42 complete! \tAverage Loss:  -57333.39006896552\n",
            "\tEpoch 43 complete! \tAverage Loss:  -57324.47489655172\n",
            "\tEpoch 44 complete! \tAverage Loss:  -57338.38344827586\n",
            "\tEpoch 45 complete! \tAverage Loss:  -57327.28565517241\n",
            "\tEpoch 46 complete! \tAverage Loss:  -57349.67655172414\n",
            "\tEpoch 47 complete! \tAverage Loss:  -57376.04496551724\n",
            "\tEpoch 48 complete! \tAverage Loss:  -57482.30455172414\n",
            "\tEpoch 49 complete! \tAverage Loss:  -57516.042758620686\n",
            "\tEpoch 50 complete! \tAverage Loss:  -57504.20717241379\n",
            "\tEpoch 51 complete! \tAverage Loss:  -57520.097655172416\n",
            "\tEpoch 52 complete! \tAverage Loss:  -57507.15917241379\n",
            "\tEpoch 53 complete! \tAverage Loss:  -57511.053655172414\n",
            "\tEpoch 54 complete! \tAverage Loss:  -57524.38468965517\n",
            "\tEpoch 55 complete! \tAverage Loss:  -57550.59448275862\n",
            "\tEpoch 56 complete! \tAverage Loss:  -57597.997931034486\n",
            "\tEpoch 57 complete! \tAverage Loss:  -57620.65186206897\n",
            "\tEpoch 58 complete! \tAverage Loss:  -57628.700689655176\n",
            "\tEpoch 59 complete! \tAverage Loss:  -57655.397103448275\n",
            "\tEpoch 60 complete! \tAverage Loss:  -57699.41724137931\n",
            "\tEpoch 61 complete! \tAverage Loss:  -57689.01572413793\n",
            "\tEpoch 62 complete! \tAverage Loss:  -57682.83255172414\n",
            "\tEpoch 63 complete! \tAverage Loss:  -57694.05144827586\n",
            "\tEpoch 64 complete! \tAverage Loss:  -57720.307586206894\n",
            "\tEpoch 65 complete! \tAverage Loss:  -57721.019586206894\n",
            "\tEpoch 66 complete! \tAverage Loss:  -57723.99462068966\n",
            "\tEpoch 67 complete! \tAverage Loss:  -57756.39034482759\n",
            "\tEpoch 68 complete! \tAverage Loss:  -57729.793103448275\n",
            "\tEpoch 69 complete! \tAverage Loss:  -57770.0755862069\n",
            "\tEpoch 70 complete! \tAverage Loss:  -57773.466482758624\n",
            "\tEpoch 71 complete! \tAverage Loss:  -57776.546068965516\n",
            "\tEpoch 72 complete! \tAverage Loss:  -57764.88896551724\n",
            "\tEpoch 73 complete! \tAverage Loss:  -57770.70579310345\n",
            "\tEpoch 74 complete! \tAverage Loss:  -57792.01986206896\n",
            "\tEpoch 75 complete! \tAverage Loss:  -57802.09737931035\n",
            "\tEpoch 76 complete! \tAverage Loss:  -57788.292137931036\n",
            "\tEpoch 77 complete! \tAverage Loss:  -57791.766344827585\n",
            "\tEpoch 78 complete! \tAverage Loss:  -57803.87489655172\n",
            "\tEpoch 79 complete! \tAverage Loss:  -57789.27944827586\n",
            "\tEpoch 80 complete! \tAverage Loss:  -57800.2951724138\n",
            "\tEpoch 81 complete! \tAverage Loss:  -57773.74924137931\n",
            "\tEpoch 82 complete! \tAverage Loss:  -57802.37627586207\n",
            "\tEpoch 83 complete! \tAverage Loss:  -57801.897103448275\n",
            "\tEpoch 84 complete! \tAverage Loss:  -57803.40082758621\n",
            "\tEpoch 85 complete! \tAverage Loss:  -57817.515034482756\n",
            "\tEpoch 86 complete! \tAverage Loss:  -57851.793379310344\n",
            "\tEpoch 87 complete! \tAverage Loss:  -57922.349931034485\n",
            "\tEpoch 88 complete! \tAverage Loss:  -58075.8564137931\n",
            "\tEpoch 89 complete! \tAverage Loss:  -58084.90027586207\n",
            "\tEpoch 90 complete! \tAverage Loss:  -58094.7835862069\n",
            "\tEpoch 91 complete! \tAverage Loss:  -58113.19572413793\n",
            "\tEpoch 92 complete! \tAverage Loss:  -58156.46634482759\n",
            "\tEpoch 93 complete! \tAverage Loss:  -58200.97448275862\n",
            "\tEpoch 94 complete! \tAverage Loss:  -58197.46096551724\n",
            "\tEpoch 95 complete! \tAverage Loss:  -58209.57820689655\n",
            "\tEpoch 96 complete! \tAverage Loss:  -58222.411586206894\n",
            "\tEpoch 97 complete! \tAverage Loss:  -58245.41944827586\n",
            "\tEpoch 98 complete! \tAverage Loss:  -58282.23172413793\n",
            "\tEpoch 99 complete! \tAverage Loss:  -58286.33393103448\n",
            "\tEpoch 100 complete! \tAverage Loss:  -58320.7124137931\n",
            "Finish!!\n"
          ]
        }
      ]
    }
  ]
}