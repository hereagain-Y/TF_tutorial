{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_615.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1thrlhNmcusSBm4PXTw1IVDeOfFemuFf-",
      "authorship_tag": "ABX9TyMvi/iuXSqV9n2pL+O+Bjab",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hereagain-Y/TF_tutorial/blob/main/VAE_615.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7-ylF3xzCcZm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"/content/drive/MyDrive/DL/VAE/rep_00148.tsv\",\n",
        "            sep=\"\\t\")\n",
        "comp_seq=data[\"amino_acid\"].tolist()\n",
        "seq='IHTGCYRTQYNQGNK'\n",
        "print(comp_seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDcHSy4BCkUW",
        "outputId": "a364bc34-afe5-4992-fadf-b9129f666fb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['WPPHRNQASTKCLNNMQ', 'PLQEDGMNNDH', 'MFRQLIPEIIC', 'TLRQMCHKHKVV', 'SNRWDGRSIWHVDKAYH', 'EHMCIQGNFLAGLVS', 'YITNHHNVPLIAFWY', 'NYLHGNGSWSVSGIPF', 'YIAFWMIDMG', 'LAEEMGRCPI', 'PCRWRHCEHTL', 'CEPWAGGVKYK', 'KDQGVWCHNLPDKYI', 'VSHCDKCREGQAISDMEHS', 'QCKFCTRDCWDSG', 'DYGGVGFTNCMY', 'VNDEMDWETQHGGVHQKT', 'RDCYQIKPHDINPTYE', 'SMKVQDFPTPSC', 'RWCLVKRIGHCMD', 'HMLQFHYQRGT', 'DPVECCWHLQLE', 'MTHRANSCGSK', 'TETIFIHWFSNIRQE', 'YVKLKTPLKGYFRYYIE', 'ETGMTHNWEVWSLHDYI', 'YYQIRGYRKDVMM', 'AHGNMAVKHSRDKPTWYM', 'TKLPRVTFWICGQPER', 'QSMSTMFAMKKQ', 'WCLHEQDSTWRWVAVNLE', 'YLYEATLMKR', 'GSCHIPMKYCWHTRGCFHN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ONE_HOT = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
        "                   'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}"
      ],
      "metadata": {
        "id": "4QFoT0kyCyNq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load tumorCDR3 data"
      ],
      "metadata": {
        "id": "UWTUQARYGZR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = []\n",
        "my_file = open(\"/content/drive/My Drive/DL/VAE/NormalCDR3.txt\", \"r\")\n",
        "content_list = my_file.read().splitlines()\n",
        "content_list=np.array(content_list)\n",
        "print(content_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sdzy4Q9GYu3",
        "outputId": "8a71a2ed-9843-4926-f604-d3b37f34e9b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CASSLKPNTEAFF' 'CASSAHIANYGYTF' 'CASSPRPNTEAFF' ... 'CASSVDVGYEQYF'\n",
            " 'CASRLAGQETQYF' 'CASSVVPNTEAFF']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d= {} # bipphysics dictionary\n",
        "with open(\"/content/drive/My Drive/DL/VAE/AAidx_PCA.txt\") as f:\n",
        "    next(f)\n",
        "    for line in f.readlines():\n",
        "        line=line.strip().split('\\t') #\n",
        "        AA=line[0]\n",
        "        tag=0\n",
        "        values=[]\n",
        "        for PC in line[1:]:\n",
        "            values.append(float(PC))\n",
        "        if tag==1: \n",
        "            continue\n",
        "        d[AA]=values\n",
        "d # AA\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP7LBLHxC49D",
        "outputId": "f6d5be17-4d8f-4498-d65c-571c1b54be33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': [-0.970906026408001,\n",
              "  -0.323681347371203,\n",
              "  15.7206518221322,\n",
              "  -0.508840655632075,\n",
              "  3.7402185826504,\n",
              "  -0.778796812231878,\n",
              "  3.38667656384665,\n",
              "  -0.913062747756598,\n",
              "  3.00614750780962,\n",
              "  -2.32919906002667,\n",
              "  0.787021897929849,\n",
              "  -1.8743796601396,\n",
              "  1.53335388267698,\n",
              "  1.3444609979428,\n",
              "  3.35815928624532],\n",
              " 'C': [-8.36918251369427,\n",
              "  8.3031934498954,\n",
              "  -6.6196694564762,\n",
              "  13.8734139891373,\n",
              "  8.59531323791867,\n",
              "  9.79914818190207,\n",
              "  1.26911186662177,\n",
              "  -4.63752901352054,\n",
              "  -0.983921565990354,\n",
              "  4.3634243590938,\n",
              "  2.34365256090458,\n",
              "  0.343445840449375,\n",
              "  1.18290876288827,\n",
              "  0.0132558803925318,\n",
              "  -0.696621502763574],\n",
              " 'D': [18.126265894029,\n",
              "  -2.147381547322,\n",
              "  -0.252169840858429,\n",
              "  2.31366106621094,\n",
              "  6.52406371824222,\n",
              "  -4.88386469358817,\n",
              "  -9.13828093975311,\n",
              "  -0.710243755381537,\n",
              "  -2.10295526053507,\n",
              "  -1.27865374462701,\n",
              "  1.48217397992869,\n",
              "  1.43032328912492,\n",
              "  -4.04631610616374,\n",
              "  5.72388975986284,\n",
              "  -1.78674912966952],\n",
              " 'E': [12.0315987413064,\n",
              "  -13.3012190783805,\n",
              "  8.27995381662754,\n",
              "  -2.19638859736273,\n",
              "  8.48308970891187,\n",
              "  -2.02004094269803,\n",
              "  -4.58165905380202,\n",
              "  -2.68526504570203,\n",
              "  -2.60424798324428,\n",
              "  -0.13228263087124,\n",
              "  -0.239494399428607,\n",
              "  -2.40393484763327,\n",
              "  4.18027450184331,\n",
              "  -4.40432038586252,\n",
              "  0.239790654440735],\n",
              " 'F': [-18.5957285875671,\n",
              "  0.922355086860979,\n",
              "  -3.3174326841828,\n",
              "  -2.5217456210048,\n",
              "  -0.451259225474838,\n",
              "  -4.09153376370189,\n",
              "  -0.39932096393613,\n",
              "  2.41122440369446,\n",
              "  -1.08364129945629,\n",
              "  -1.47072537310938,\n",
              "  3.19432867781133,\n",
              "  2.04374526745972,\n",
              "  0.29441083041113,\n",
              "  -3.18840008751722,\n",
              "  -6.45603094467351],\n",
              " 'G': [14.8392087276565,\n",
              "  19.241389058527,\n",
              "  5.93438828703323,\n",
              "  5.6673840103989,\n",
              "  -5.81883134135928,\n",
              "  -8.78866739105715,\n",
              "  5.73914234836355,\n",
              "  -4.72849217650846,\n",
              "  -4.20333218578641,\n",
              "  -1.56492844517094,\n",
              "  -1.37450168764811,\n",
              "  -0.612538358248982,\n",
              "  0.661713396711836,\n",
              "  -0.699213715441531,\n",
              "  -1.40639533631965],\n",
              " 'H': [0.688054393776236,\n",
              "  -6.17956596015547,\n",
              "  -6.80550603963819,\n",
              "  3.94746531151465,\n",
              "  1.33297594422066,\n",
              "  -0.512113037063432,\n",
              "  4.73783089151909,\n",
              "  8.74722376658272,\n",
              "  -3.12401236141799,\n",
              "  -1.5759818605281,\n",
              "  1.98977265617852,\n",
              "  -7.78869789136917,\n",
              "  0.99852322205236,\n",
              "  1.37575734278336,\n",
              "  -0.431200176359418],\n",
              " 'I': [-20.3408467684748,\n",
              "  4.14384692950592,\n",
              "  3.86394279267726,\n",
              "  -3.40504490796988,\n",
              "  -2.78992753593753,\n",
              "  1.46082574288602,\n",
              "  -4.07088528387306,\n",
              "  0.583871517452551,\n",
              "  -3.41337954154765,\n",
              "  1.84625632023229,\n",
              "  -3.12082911814009,\n",
              "  -1.14873131307641,\n",
              "  -4.65093113761947,\n",
              "  -1.64689363496071,\n",
              "  1.46078506741182],\n",
              " 'K': [11.7065885118909,\n",
              "  -13.6448857650545,\n",
              "  2.13621858404915,\n",
              "  -2.01605285265223,\n",
              "  -6.38068619478022,\n",
              "  0.812260806672307,\n",
              "  4.37857191762294,\n",
              "  -1.64960819597265,\n",
              "  1.42540471981008,\n",
              "  8.60094140470994,\n",
              "  -2.91415960161008,\n",
              "  -1.63197862122119,\n",
              "  -1.76542325626818,\n",
              "  2.935935422477,\n",
              "  -2.78651304601605],\n",
              " 'L': [-17.6362377212498,\n",
              "  -0.352395967499455,\n",
              "  11.8392431711762,\n",
              "  -5.3754085147865,\n",
              "  -1.15954659421633,\n",
              "  -1.97240236972529,\n",
              "  0.85513949646815,\n",
              "  0.530557404169316,\n",
              "  2.12017619683102,\n",
              "  4.05167587537177,\n",
              "  7.06745698911776,\n",
              "  2.57547229188008,\n",
              "  1.45479039853073,\n",
              "  1.28288187447289,\n",
              "  -0.265547876534496],\n",
              " 'M': [-15.5965459439888,\n",
              "  -5.74596901310561,\n",
              "  1.05654897999068,\n",
              "  1.82581269817027,\n",
              "  6.65236588032325,\n",
              "  -1.21996347058177,\n",
              "  6.87749570872555,\n",
              "  2.80480871298562,\n",
              "  -1.90746691338316,\n",
              "  -3.59099156149383,\n",
              "  -4.05859459604521,\n",
              "  5.8399736792111,\n",
              "  -2.03857504059584,\n",
              "  2.47274256865339,\n",
              "  1.55383648787537],\n",
              " 'N': [14.8697608823814,\n",
              "  1.57062419351366,\n",
              "  -3.62321751338539,\n",
              "  5.63672397606222,\n",
              "  -1.78396130934604,\n",
              "  -3.6273682169564,\n",
              "  -1.44652457140288,\n",
              "  5.39945262367184,\n",
              "  0.876168782317429,\n",
              "  4.11287759691914,\n",
              "  1.91381611263301,\n",
              "  2.42063270400044,\n",
              "  -3.81386421602415,\n",
              "  -5.24259600349578,\n",
              "  3.97131233439305],\n",
              " 'P': [16.2170472324125,\n",
              "  15.0912703606343,\n",
              "  -8.72320617074907,\n",
              "  -18.7775000606303,\n",
              "  6.46478124101104,\n",
              "  4.5624497624343,\n",
              "  3.05557798937367,\n",
              "  0.156904082894837,\n",
              "  -0.669299994747313,\n",
              "  0.733899404673533,\n",
              "  0.395332101489257,\n",
              "  -0.16512657530992,\n",
              "  -0.43811541502197,\n",
              "  0.0520347513015065,\n",
              "  0.505554282481843],\n",
              " 'Q': [7.92109649541505,\n",
              "  -8.69666505891922,\n",
              "  -0.597016944992033,\n",
              "  0.020462956968454,\n",
              "  0.930263968878804,\n",
              "  2.56565473960784,\n",
              "  1.14554765946938,\n",
              "  0.50500247292699,\n",
              "  0.714673438110632,\n",
              "  0.0443860553021852,\n",
              "  -6.00954372483693,\n",
              "  3.89329375996293,\n",
              "  2.44305792954492,\n",
              "  -2.76002277530737,\n",
              "  -1.6987171966435],\n",
              " 'R': [8.53814624869308,\n",
              "  -13.7874583583686,\n",
              "  -4.77062839503559,\n",
              "  -1.164493779805,\n",
              "  -9.46561382764551,\n",
              "  6.79244266108082,\n",
              "  1.98971003980151,\n",
              "  -5.15274046492092,\n",
              "  -2.93772087687042,\n",
              "  -5.89206950892258,\n",
              "  5.26993696032714,\n",
              "  1.61036613224437,\n",
              "  -1.95208182953383,\n",
              "  -0.644089995894696,\n",
              "  1.53934308642653],\n",
              " 'S': [11.8527409926327,\n",
              "  6.88460213778818,\n",
              "  2.96725950854065,\n",
              "  3.62901850422307,\n",
              "  -2.8861931213862,\n",
              "  2.32382186250058,\n",
              "  -1.33830204027272,\n",
              "  3.31676758103915,\n",
              "  6.26373174858307,\n",
              "  -0.897892655616456,\n",
              "  0.73941400126498,\n",
              "  1.15021804640236,\n",
              "  1.36104635450631,\n",
              "  1.1330584751681,\n",
              "  1.16395609475909],\n",
              " 'T': [4.53029864849734,\n",
              "  5.12654936011017,\n",
              "  1.43605965188805,\n",
              "  1.76843719960302,\n",
              "  -3.39334851766157,\n",
              "  5.24360932225774,\n",
              "  -3.36448142640725,\n",
              "  2.20062621411236,\n",
              "  6.52685306875626,\n",
              "  -4.74609137120591,\n",
              "  -2.03988323807275,\n",
              "  -0.650552894793387,\n",
              "  0.192728842753563,\n",
              "  -0.254397918567873,\n",
              "  -3.25573314184742],\n",
              " 'V': [-16.0144131878424,\n",
              "  5.49304582237124,\n",
              "  7.34505768240461,\n",
              "  -0.712585326451388,\n",
              "  -4.03171244582313,\n",
              "  5.6074500557407,\n",
              "  -4.73473403768965,\n",
              "  -0.446559975736051,\n",
              "  -2.68117513178825,\n",
              "  -0.631767637282524,\n",
              "  -2.98159018765122,\n",
              "  -3.67133045753567,\n",
              "  -1.09976811360973,\n",
              "  0.241982575839527,\n",
              "  0.769513312384431],\n",
              " 'W': [-16.2976860162016,\n",
              "  -3.90769937000452,\n",
              "  -13.7925548759183,\n",
              "  -0.843257985763393,\n",
              "  2.33264712806999,\n",
              "  -8.04187275898055,\n",
              "  0.501864943465053,\n",
              "  -6.42108914752241,\n",
              "  7.23273529548279,\n",
              "  -1.12521690586036,\n",
              "  -1.01561523945109,\n",
              "  -3.54186465784877,\n",
              "  -1.94456110459367,\n",
              "  -0.814384632830866,\n",
              "  1.37250825023202],\n",
              " 'Y': [-7.4992600032643,\n",
              "  1.31004506697425,\n",
              "  -12.0779223752836,\n",
              "  -1.16106141023061,\n",
              "  -6.89463929659625,\n",
              "  -3.23103967849783,\n",
              "  -4.86248110814052,\n",
              "  0.688151743491353,\n",
              "  -2.45473764293373,\n",
              "  1.48233973841234,\n",
              "  -1.42869414470103,\n",
              "  2.18166426644109,\n",
              "  7.4468280975112,\n",
              "  3.07831950098463,\n",
              "  2.84874949417693]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max length\n",
        "#comp_seq=data[\"amino_acid\"].tolist()\n",
        "comp_seq=content_list\n",
        "print(comp_seq)\n",
        "max_len=-1 \n",
        "\n",
        "for AA in comp_seq:\n",
        "    if len(AA)>max_len:\n",
        "        max_len=len(AA)\n",
        "        #res=AA\n",
        "print(max_len )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxYBWURNC_9Z",
        "outputId": "a806733f-26e2-4a49-bb45-52c2aacf9619"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CASSLKPNTEAFF' 'CASSAHIANYGYTF' 'CASSPRPNTEAFF' ... 'CASSVDVGYEQYF'\n",
            " 'CASRLAGQETQYF' 'CASSVVPNTEAFF']\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PC_length=len(d['C'])\n",
        "def AAindexEncoding(Seq):\n",
        "    length_seq=len(Seq)\n",
        "    global max_len\n",
        "    AAE=np.zeros([max_len,15])\n",
        "    if length_seq<max_len:\n",
        "        for amino in range(length_seq):\n",
        "            AA=Seq[amino]# \n",
        "            AAE[amino,]=d[AA] # add PC value \n",
        "            \n",
        "        for amino in range(length_seq,max_len):\n",
        "            AAE[amino,]=np.zeros(15)\n",
        "    else: \n",
        "        for amino in range(length_seq):\n",
        "            AA=Seq[amino]# \n",
        "            AAE[amino,]=d[AA]\n",
        "        \n",
        "    #AAE=np.transpose(AAE.astype(np.float32)) # row as PC. and column as AA sequence \n",
        "    return AAE \n",
        "  \n",
        "def GetFeatures(file):\n",
        "    #sequence=file['amino_acid'].tolist()\n",
        "    #sequence=np.array(sequence)\n",
        "    #sequence = file.read().splitlines()\n",
        "    #sequence=np.array(sequence)\n",
        "    hot_encode=[]\n",
        "    for seq in file:\n",
        "        hot_encode.append(AAindexEncoding(seq))\n",
        "    hot_encode=np.array(hot_encode)\n",
        "    result=np.array(hot_encode)\n",
        "    return(result) # dimension: number of sequence [15*length(sequence)]\n",
        "\n",
        "\n",
        "r1=GetFeatures(content_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "5b3L9CNdDIb1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "nfKE5VwJDONG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "Q8pBPD42GEvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trabsform \n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
        "r1_transform=torch.from_numpy(r1)\n",
        "r1_transform=r1_transform.float()\n",
        "train_ds, test_ds = torch.utils.data.random_split(r1_transform, (30000, 10000))\n",
        "print(train_ds, test_ds)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_ds, batch_size=1000)\n",
        "test_loader  = DataLoader(dataset=test_ds,  batch_size=1000)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esy5kIOXGEht",
        "outputId": "c7878ad5-8059-4971-e247-42b859ce80d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataset.Subset object at 0x7f274024bf10> <torch.utils.data.dataset.Subset object at 0x7f2740089c10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set paramters:\n",
        "cuda = True\n",
        "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "x_dim=285 # 19*15\n",
        "hidden_dim = 256\n",
        "latent_dim = 128\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "HizXfI1blIlq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "        \n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        \n",
        "        self.training = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_       = self.LeakyReLU(self.FC_input(x))\n",
        "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
        "        mean     = self.FC_mean(h_)\n",
        "        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance \n",
        "                                                       #             (i.e., parateters of simple tractable normal distribution \"q\"\n",
        "        \n",
        "        return mean, log_var"
      ],
      "metadata": {
        "id": "f6l50xr0F9BQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
        "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
        "        \n",
        "        x_hat = torch.sigmoid(self.FC_output(h))\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "_xKOYca8GGS0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "        \n",
        "    def reparameterization(self, mean, var):\n",
        "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
        "        z = mean + var*epsilon                          # reparameterization trick\n",
        "        return z\n",
        "        \n",
        "                \n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.Encoder(x)\n",
        "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var) ]\n",
        "        x_hat            = self.Decoder(z)\n",
        "        \n",
        "        return x_hat, mean, log_var"
      ],
      "metadata": {
        "id": "oftbvF8pGIEp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
      ],
      "metadata": {
        "id": "9GrhkqDjGJq-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "x9DQJN6wrjU-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training VAE...\")\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    overall_loss = 0\n",
        "    overall_testloss= 0\n",
        "    for batch_idx, x in enumerate(train_loader):\n",
        "        x = x.view(1000, x_dim)\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_hat, mean, log_var = model(x)\n",
        "        loss = loss_function(x, x_hat, mean, log_var)\n",
        "        overall_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      #test loss\n",
        "    for batch_idx, x in enumerate(test_loader):\n",
        "        x = x.view(1000, x_dim)\n",
        "        x = x.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred, mean, log_var = model(x)\n",
        "        test_loss = loss_function(x, pred, mean, log_var)\n",
        "        overall_testloss += test_loss.item()\n",
        "        \n",
        "        \n",
        "        test_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / (batch_idx*batch_size),\n",
        "         \"\\tAverage Test Loss: \" , overall_testloss/(batch_idx*batch_size))\n",
        "    \n",
        "print(\"Finish!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQKesrMHGLLd",
        "outputId": "19de68d0-a900-4bd0-f238-fed5653a61bf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training VAE...\n",
            "\tEpoch 1 complete! \tAverage Loss:  -57696.313388237846 \tAverage Test Loss:  -29315.97288888889\n",
            "\tEpoch 2 complete! \tAverage Loss:  -94197.88555555555 \tAverage Test Loss:  -32270.57222222222\n",
            "\tEpoch 3 complete! \tAverage Loss:  -100985.95244444444 \tAverage Test Loss:  -35979.989555555556\n",
            "\tEpoch 4 complete! \tAverage Loss:  -112430.35133333334 \tAverage Test Loss:  -38490.97111111111\n",
            "\tEpoch 5 complete! \tAverage Loss:  -118853.80488888889 \tAverage Test Loss:  -40648.09066666666\n",
            "\tEpoch 6 complete! \tAverage Loss:  -125092.37333333334 \tAverage Test Loss:  -43061.346666666665\n",
            "\tEpoch 7 complete! \tAverage Loss:  -131965.78 \tAverage Test Loss:  -44941.95111111111\n",
            "\tEpoch 8 complete! \tAverage Loss:  -136797.0231111111 \tAverage Test Loss:  -46111.64844444444\n",
            "\tEpoch 9 complete! \tAverage Loss:  -139306.04666666666 \tAverage Test Loss:  -46834.26222222222\n",
            "\tEpoch 10 complete! \tAverage Loss:  -141433.85866666667 \tAverage Test Loss:  -47579.69733333333\n",
            "\tEpoch 11 complete! \tAverage Loss:  -143529.70133333333 \tAverage Test Loss:  -48243.53244444444\n",
            "\tEpoch 12 complete! \tAverage Loss:  -145796.06577777778 \tAverage Test Loss:  -48943.440444444444\n",
            "\tEpoch 13 complete! \tAverage Loss:  -147163.304 \tAverage Test Loss:  -49252.912444444446\n",
            "\tEpoch 14 complete! \tAverage Loss:  -148245.652 \tAverage Test Loss:  -49608.96444444444\n",
            "\tEpoch 15 complete! \tAverage Loss:  -149083.52133333334 \tAverage Test Loss:  -49836.99733333333\n",
            "\tEpoch 16 complete! \tAverage Loss:  -149697.58933333334 \tAverage Test Loss:  -50055.464\n",
            "\tEpoch 17 complete! \tAverage Loss:  -150212.48666666666 \tAverage Test Loss:  -50217.22977777778\n",
            "\tEpoch 18 complete! \tAverage Loss:  -150742.95733333332 \tAverage Test Loss:  -50374.523555555556\n",
            "\tEpoch 19 complete! \tAverage Loss:  -151140.09511111112 \tAverage Test Loss:  -50585.00977777778\n",
            "\tEpoch 20 complete! \tAverage Loss:  -151736.37155555555 \tAverage Test Loss:  -50699.99733333333\n",
            "\tEpoch 21 complete! \tAverage Loss:  -152196.36222222223 \tAverage Test Loss:  -50832.19733333333\n",
            "\tEpoch 22 complete! \tAverage Loss:  -152557.2417777778 \tAverage Test Loss:  -50973.12577777778\n",
            "\tEpoch 23 complete! \tAverage Loss:  -153039.712 \tAverage Test Loss:  -51157.01155555555\n",
            "\tEpoch 24 complete! \tAverage Loss:  -153472.93733333334 \tAverage Test Loss:  -51283.23333333333\n",
            "\tEpoch 25 complete! \tAverage Loss:  -153904.71644444444 \tAverage Test Loss:  -51481.339555555554\n",
            "\tEpoch 26 complete! \tAverage Loss:  -154525.5991111111 \tAverage Test Loss:  -51636.354222222224\n",
            "\tEpoch 27 complete! \tAverage Loss:  -154858.30577777777 \tAverage Test Loss:  -51770.80711111111\n",
            "\tEpoch 28 complete! \tAverage Loss:  -155230.596 \tAverage Test Loss:  -51851.67955555556\n",
            "\tEpoch 29 complete! \tAverage Loss:  -155573.29244444444 \tAverage Test Loss:  -51974.79555555555\n",
            "\tEpoch 30 complete! \tAverage Loss:  -156051.51155555557 \tAverage Test Loss:  -52126.491555555556\n",
            "\tEpoch 31 complete! \tAverage Loss:  -156337.8342222222 \tAverage Test Loss:  -52242.329333333335\n",
            "\tEpoch 32 complete! \tAverage Loss:  -156640.07377777778 \tAverage Test Loss:  -52313.32488888889\n",
            "\tEpoch 33 complete! \tAverage Loss:  -156895.584 \tAverage Test Loss:  -52405.724444444444\n",
            "\tEpoch 34 complete! \tAverage Loss:  -157140.86133333333 \tAverage Test Loss:  -52510.322222222225\n",
            "\tEpoch 35 complete! \tAverage Loss:  -157511.96711111112 \tAverage Test Loss:  -52621.08177777778\n",
            "\tEpoch 36 complete! \tAverage Loss:  -157822.22977777777 \tAverage Test Loss:  -52730.966222222225\n",
            "\tEpoch 37 complete! \tAverage Loss:  -158077.66977777777 \tAverage Test Loss:  -52812.74711111111\n",
            "\tEpoch 38 complete! \tAverage Loss:  -158302.9231111111 \tAverage Test Loss:  -52882.11866666667\n",
            "\tEpoch 39 complete! \tAverage Loss:  -158600.04888888888 \tAverage Test Loss:  -52992.22755555555\n",
            "\tEpoch 40 complete! \tAverage Loss:  -158848.93644444444 \tAverage Test Loss:  -53055.221333333335\n",
            "\tEpoch 41 complete! \tAverage Loss:  -159121.43022222223 \tAverage Test Loss:  -53131.53288888889\n",
            "\tEpoch 42 complete! \tAverage Loss:  -159424.27688888888 \tAverage Test Loss:  -53241.92844444444\n",
            "\tEpoch 43 complete! \tAverage Loss:  -159679.17955555554 \tAverage Test Loss:  -53331.07511111111\n",
            "\tEpoch 44 complete! \tAverage Loss:  -159920.72133333332 \tAverage Test Loss:  -53375.54577777778\n",
            "\tEpoch 45 complete! \tAverage Loss:  -160088.79822222222 \tAverage Test Loss:  -53442.22622222222\n",
            "\tEpoch 46 complete! \tAverage Loss:  -160253.92088888888 \tAverage Test Loss:  -53476.43955555555\n",
            "\tEpoch 47 complete! \tAverage Loss:  -160503.37555555557 \tAverage Test Loss:  -53596.89955555555\n",
            "\tEpoch 48 complete! \tAverage Loss:  -160765.99955555555 \tAverage Test Loss:  -53733.148888888885\n",
            "\tEpoch 49 complete! \tAverage Loss:  -161084.57555555555 \tAverage Test Loss:  -53779.72266666667\n",
            "\tEpoch 50 complete! \tAverage Loss:  -161576.8351111111 \tAverage Test Loss:  -54193.440444444444\n",
            "\tEpoch 51 complete! \tAverage Loss:  -162759.96577777778 \tAverage Test Loss:  -54375.91688888889\n",
            "\tEpoch 52 complete! \tAverage Loss:  -163063.70533333335 \tAverage Test Loss:  -54437.48\n",
            "\tEpoch 53 complete! \tAverage Loss:  -163265.79777777777 \tAverage Test Loss:  -54475.41066666667\n",
            "\tEpoch 54 complete! \tAverage Loss:  -163567.6648888889 \tAverage Test Loss:  -54565.858222222225\n",
            "\tEpoch 55 complete! \tAverage Loss:  -163936.43866666665 \tAverage Test Loss:  -54699.444\n",
            "\tEpoch 56 complete! \tAverage Loss:  -164095.28044444445 \tAverage Test Loss:  -54796.847555555556\n",
            "\tEpoch 57 complete! \tAverage Loss:  -164349.92355555555 \tAverage Test Loss:  -54834.519555555555\n",
            "\tEpoch 58 complete! \tAverage Loss:  -164419.94444444444 \tAverage Test Loss:  -54882.81422222222\n",
            "\tEpoch 59 complete! \tAverage Loss:  -164603.64888888889 \tAverage Test Loss:  -54934.101777777774\n",
            "\tEpoch 60 complete! \tAverage Loss:  -164723.33244444444 \tAverage Test Loss:  -54947.91288888889\n",
            "\tEpoch 61 complete! \tAverage Loss:  -164820.1737777778 \tAverage Test Loss:  -54989.26888888889\n",
            "\tEpoch 62 complete! \tAverage Loss:  -164988.276 \tAverage Test Loss:  -55067.54755555555\n",
            "\tEpoch 63 complete! \tAverage Loss:  -165108.832 \tAverage Test Loss:  -55138.574222222225\n",
            "\tEpoch 64 complete! \tAverage Loss:  -165479.16844444445 \tAverage Test Loss:  -55256.009333333335\n",
            "\tEpoch 65 complete! \tAverage Loss:  -165661.66222222222 \tAverage Test Loss:  -55320.92222222222\n",
            "\tEpoch 66 complete! \tAverage Loss:  -165893.8728888889 \tAverage Test Loss:  -55421.717777777776\n",
            "\tEpoch 67 complete! \tAverage Loss:  -166224.7751111111 \tAverage Test Loss:  -55523.66622222222\n",
            "\tEpoch 68 complete! \tAverage Loss:  -166504.42177777778 \tAverage Test Loss:  -55613.397333333334\n",
            "\tEpoch 69 complete! \tAverage Loss:  -166725.31333333332 \tAverage Test Loss:  -55653.40355555555\n",
            "\tEpoch 70 complete! \tAverage Loss:  -166950.75555555554 \tAverage Test Loss:  -55737.85955555556\n",
            "\tEpoch 71 complete! \tAverage Loss:  -167226.85733333332 \tAverage Test Loss:  -55885.544444444444\n",
            "\tEpoch 72 complete! \tAverage Loss:  -167576.456 \tAverage Test Loss:  -55899.23288888889\n",
            "\tEpoch 73 complete! \tAverage Loss:  -167803.57777777777 \tAverage Test Loss:  -55988.695555555554\n",
            "\tEpoch 74 complete! \tAverage Loss:  -168031.1311111111 \tAverage Test Loss:  -56074.35688888889\n",
            "\tEpoch 75 complete! \tAverage Loss:  -168217.7288888889 \tAverage Test Loss:  -56117.58355555555\n",
            "\tEpoch 76 complete! \tAverage Loss:  -168446.91955555556 \tAverage Test Loss:  -56255.41066666667\n",
            "\tEpoch 77 complete! \tAverage Loss:  -168732.94755555555 \tAverage Test Loss:  -56328.635111111114\n",
            "\tEpoch 78 complete! \tAverage Loss:  -168937.1511111111 \tAverage Test Loss:  -56390.905333333336\n",
            "\tEpoch 79 complete! \tAverage Loss:  -169093.86933333334 \tAverage Test Loss:  -56418.78311111111\n",
            "\tEpoch 80 complete! \tAverage Loss:  -169155.69066666666 \tAverage Test Loss:  -56448.01733333334\n",
            "\tEpoch 81 complete! \tAverage Loss:  -169339.63377777778 \tAverage Test Loss:  -56489.29377777778\n",
            "\tEpoch 82 complete! \tAverage Loss:  -169367.81066666666 \tAverage Test Loss:  -56560.091555555555\n",
            "\tEpoch 83 complete! \tAverage Loss:  -169461.8031111111 \tAverage Test Loss:  -56579.17333333333\n",
            "\tEpoch 84 complete! \tAverage Loss:  -169534.97333333333 \tAverage Test Loss:  -56612.47066666667\n",
            "\tEpoch 85 complete! \tAverage Loss:  -169687.588 \tAverage Test Loss:  -56626.49555555556\n",
            "\tEpoch 86 complete! \tAverage Loss:  -169760.64711111112 \tAverage Test Loss:  -56696.962222222224\n",
            "\tEpoch 87 complete! \tAverage Loss:  -169871.95022222222 \tAverage Test Loss:  -56732.98711111111\n",
            "\tEpoch 88 complete! \tAverage Loss:  -170048.79066666667 \tAverage Test Loss:  -56776.72266666667\n",
            "\tEpoch 89 complete! \tAverage Loss:  -170173.01155555557 \tAverage Test Loss:  -56824.84933333333\n",
            "\tEpoch 90 complete! \tAverage Loss:  -170242.74 \tAverage Test Loss:  -56861.572\n",
            "\tEpoch 91 complete! \tAverage Loss:  -170460.59644444444 \tAverage Test Loss:  -56912.61288888889\n",
            "\tEpoch 92 complete! \tAverage Loss:  -170593.68622222223 \tAverage Test Loss:  -56967.96\n",
            "\tEpoch 93 complete! \tAverage Loss:  -170704.00355555557 \tAverage Test Loss:  -57020.26577777778\n",
            "\tEpoch 94 complete! \tAverage Loss:  -170845.24666666667 \tAverage Test Loss:  -57010.71955555556\n",
            "\tEpoch 95 complete! \tAverage Loss:  -170909.18666666668 \tAverage Test Loss:  -57027.85511111111\n",
            "\tEpoch 96 complete! \tAverage Loss:  -171035.71333333335 \tAverage Test Loss:  -57079.26888888889\n",
            "\tEpoch 97 complete! \tAverage Loss:  -171093.19777777776 \tAverage Test Loss:  -57101.63377777778\n",
            "\tEpoch 98 complete! \tAverage Loss:  -171169.50488888889 \tAverage Test Loss:  -57125.884\n",
            "\tEpoch 99 complete! \tAverage Loss:  -171262.4448888889 \tAverage Test Loss:  -57148.70444444445\n",
            "\tEpoch 100 complete! \tAverage Loss:  -171344.00488888889 \tAverage Test Loss:  -57185.950666666664\n",
            "Finish!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create sequence "
      ],
      "metadata": {
        "id": "2EqdZOeXm9Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hY7kifaNm8oa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}